buildscript {
  repositories {
    mavenCentral()
  }
  dependencies {
    classpath 'org.codehaus.groovy.modules.http-builder:http-builder:0.7'
  }
}

import groovy.json.JsonSlurper
import groovy.xml.XmlUtil
import groovyx.net.http.HTTPBuilder
import groovyx.net.http.Method

def parentIdentifier
def sources = [
    [
        name             : 'GptCollections',
        class            : GeoportalExtractor,
        collectionsUrl   : 'http://data.nodc.noaa.gov/geoportal/rest/find/document',
        collectionsParams: [searchText: 'title%3A*%20NOT%20title%3AGHRSST'], //All collections not part of GHRSST
        requireBrowseImg : true
    ],
    [
        name             : 'GHRSST',
        class            : GeoportalExtractor,
        collectionsUrl   : 'http://data.nodc.noaa.gov/geoportal/rest/find/document',
        collectionsParams: [searchText: 'title:GHRSST%20NOT%20title:Documentation'],
        granulesUrl      : 'http://www.nodc.noaa.gov/search/granule/rest/find/document',
        granulesParams   : [searchText: "fileIdentifier:${-> parentIdentifier}*"],
        requireBrowseImg : false
    ],
    [
        name             : 'COOPS',
        class            : GeoportalExtractor,
        collectionsUrl   : 'http://data.nodc.noaa.gov/geoportal/rest/find/document',
        collectionsParams: [searchText: 'title:NWLON%20NOT%20title:Documentation'],
        granulesUrl      : 'http://www.nodc.noaa.gov/search/granule/rest/find/document',
        granulesParams   : [searchText: 'fileIdentifier%3ACO-OPS*', after: '2016-06-30'],
        requireBrowseImg : false
    ],
    [
        name             : 'DEM',
        class            : WafExtractor,
        collectionsUrl   : 'http://ngdc.noaa.gov/metadata/published/NOAA/OneStop/DEM/iso/xml/',
        collectionsParams: [:],
        requireBrowseImg : false
    ],
    [
        name: 'SAMOS',
        class: GeoportalExtractor,
        collectionsUrl: 'http://data.nodc.noaa.gov/geoportal/rest/find/document',
        collectionsParams: [searchText: 'title:SAMOS'],
        granulesUrl: 'http://data.nodc.noaa.gov/nodc/archive/metadata/test/granule/iso/samos/',
        granulesParams: [:],
        granulesClass: WafExtractor,
        requireBrowseImg : false
    ],
    [
        name: 'WOA13',
        class: GeoportalExtractor,
        collectionsUrl: 'http://data.nodc.noaa.gov/geoportal/rest/find/document',
        collectionsParams: [searchText: 'fileIdentifier:"gov.noaa.nodc:0114815"'],
        granulesUrl: 'http://data.nodc.noaa.gov/nodc/archive/metadata/test/granule/iso/woa13/',
        granulesParams: [:],
        granulesClass: WafExtractor,
        requireBrowseImg : false
    ]
]

def targets = [
    [
        name : 'LocalApi',
        class: Loader,
        url  : 'http://localhost:8097/onestop/api/metadata'
    ],
    [
        name : 'CUApi',
        class: Loader,
        url  : 'https://sciapps.colorado.edu/onestop/api/metadata'
    ],
    [
        name : 'LocalGeoportal',
        class: GeoportalLoader,
        url  : "http://localhost:8888/rest/metadata/item"
    ],
    [
        name : 'LocalFiles',
        class: Loader,
        filePath  : "/tmp"
    ]
]

// Return a closure resource for loading sources into each target
def loadToTarget( Map source, Map target ) {
  return {
    // Load collections first
    def extractor = source.class.newInstance(source.collectionsUrl, source.collectionsParams)
    def filePath = target.filePath
    def loader
    if (filePath){
      loader = target.class.newInstance(extractor, source.name, filePath, new File(filePath))
    } else {
      loader = target.class.newInstance(extractor, new HTTPBuilder(target.url))
    }
    loader.load(source.requireBrowseImg)

    // Load granules if they exist
    if (source.granulesUrl) {
      def collections = loader.collections
      collections.each { i ->
        // ParentID is only relevant to GHRSST but set here to avoid duplicate code below since granulesParams
        // is dynamically constructed for each collection
        def parentIdentifier = i

        if (source.granulesClass) {
          extractor = source.granulesClass.newInstance(source.granulesUrl, source.granulesParams)
        } else {
          extractor = source.class.newInstance(source.granulesUrl, source.granulesParams)
        }
        loader = target.class.newInstance(extractor, loader.target)
        if (source.name == 'GHRSST') {
          loader.load(parentIdentifier)
        } else {
          loader.load(source.requireBrowseImg)
        }
      }
    }
  }
}

// Creates a task to load from each source into local & server targets
targets.each { target ->
  tasks.create("etlAllTo${target.name}")
    .dependsOn( sources.collect { source ->
      tasks.create("etl${source.name}To${target.name}")
        .doFirst(loadToTarget(source, target))
  })
}

class WafExtractor implements Iterator<String> {
  private String endpointUrl
  private Map searchParams
  private Iterator wafIterator

  WafExtractor(String endpointUrl, Map searchParams = [:]) {
    this.endpointUrl = endpointUrl
    this.searchParams = searchParams

    def html = new HTTPBuilder(endpointUrl).get(searchParams)
    def xmlFiles = html.'**'.collect({ "${endpointUrl}/${it}" }).findAll({ it.endsWith('.xml') })
    this.wafIterator = xmlFiles.iterator()
  }

  @Override
  boolean hasNext() {
    return wafIterator.hasNext()
  }

  @Override
  String next() {
    return wafIterator.next()
  }
}


class GeoportalExtractor implements Iterator<String> {
  private String endpointUrl
  private Map searchParams
  private Integer pageSize = 250
  private List currentPage = []
  private Integer currentPageNum = 0
  private Integer currentIndex = 0
  private JsonSlurper jsonSlurper = new JsonSlurper()
  private gptPageRequests = 0
  private maxGptPageRequests = 2 //TODO Should be an arg passed in; Update me as needed!

  GeoportalExtractor(String endpointUrl, Map searchParams = [:]) {
    this.endpointUrl = endpointUrl
    this.searchParams = searchParams
    getNextPage()
  }

  @Override
  boolean hasNext() {
    if (currentIndex < currentPage.size()) {
      return true
    } else {
      gptPageRequests++
      println "Number of gptPageRequests: ${gptPageRequests}"
      if (gptPageRequests < maxGptPageRequests) {
        getNextPage()
        return currentPage.size() > 0
      } else {
        return false
      }
    }
  }

  @Override
  String next() {
    return hasNext() ? currentPage[currentIndex++] : null
  }

  private getNextPage() {
    try {
      currentPage = jsonSlurper.parse(buildUrl()).records.collect { record ->
        record.links.find({ it.type.contains('metadata') }).href
      }
    } catch (e) {
      println "getNextPage() failed. Go to next page..."
      sleep(3000)
    }
    currentIndex = 0
    currentPageNum++
  }

  private buildUrl() {
    def params = searchParams + [start: currentPageNum * pageSize, max: pageSize, f: 'json', orderBy: 'title']
    def queryString = params.collect({ k, v -> k + '=' + v }).join('&')
    def url = endpointUrl + '?' + queryString
    println url
    return new URL(url)
  }
}

class Loader {
  protected Iterator<String> metadataUrls
  protected Method httpMethod
  def xmlSlurper = new XmlSlurper()
  def target
  def sourceName
  def targetPath

  private collections = []

  Loader(Iterator<String> metadataUrls, HTTPBuilder target) {
    this.metadataUrls = metadataUrls
    this.target = target
    this.target.ignoreSSLIssues()
    this.httpMethod = Method.POST
  }

  Loader(Iterator<String> metadataUrls, String sourceName, String targetPath, File target) {
    this.metadataUrls = metadataUrls
    this.httpMethod = Method.POST
    this.target = target
    this.sourceName = sourceName
    this.targetPath = targetPath
  }

  void load(boolean requireBrowseImg) {
    def successes = 0
    def failures = 0
    def browseImgCollectionsFound = 0

    metadataUrls.eachWithIndex { metadata, i ->
      println "Metadata request: ${i}"
      def metadataContent
      try {
        metadataContent = new URL(metadata).text
      } catch (e) {
        println "Error downloading metadata #${i} from ${metadata}: ${e.message}"
        return // break out of current loop
      }

      if (!requireBrowseImg || hasImage(metadataContent)) {
        browseImgCollectionsFound++
        if (target in HTTPBuilder) {
          target.request(httpMethod) {
            requestContentType = 'application/xml'
            body = metadataContent
            response.success = { resp, data ->
              successes++
              println "${data.data.id} created #${i}: ${data.data.attributes.created}"
              collections.add(data.data.id.replace('gov.noaa.nodc:', ''))
            }
            response.failure = { resp, data ->
              failures++
              println "Error uploading metadata #${i}: ${data}"
            }
          }
        } else { // Target is file
          def fileName = metadata.tokenize('/').last()
          def parentDir = new File("/tmp/${sourceName}")
          parentDir.mkdirs()
          def tmpFile = new File(parentDir, "/$fileName")
          tmpFile.text = metadataContent
        }
      }

      if (requireBrowseImg) println "Found ${browseImgCollectionsFound} metadata records with browseGraphics"
      println "Load complete. Successfully loaded ${successes} of ${successes + failures} metadata records"
    }
  }

  void load(String parentIdentifier) {
    def successes = 0
    def failures = 0

    parentIdentifier = 'gov.noaa.nodc:' + parentIdentifier
    def xmlFragment = """  <gmd:parentIdentifier>
    <gco:CharacterString>${parentIdentifier}</gco:CharacterString>
  </gmd:parentIdentifier>"""
    def fragment = new XmlParser(false, false).parseText(xmlFragment)
    def root = new XmlParser(false, false)

    metadataUrls.eachWithIndex { metadata, i ->
      try {
        root.parseText(new URL(metadata).text)
        def xml = root.children()
        xml.add(fragment)
        def updatedXml = XmlUtil.serialize(root)
        target.request(httpMethod) {
          requestContentType = 'application/xml'
          body = updatedXml
          response.success = { resp, data ->
            successes++
            println "${data.data.id} created #${i}: ${data.data.attributes.created}"
          }
          response.failure = { resp, data ->
            failures++
            println "Error uploading metadata #${i} from ${metadata}: ${data}"
          }
        }
      }
      catch (e) {
        println "Error downloading metadata #${i} from ${metadata}: ${e.message}"
        return // break out of current each{} loop
      }
    }
    println "Load complete. Successfully loaded ${successes} of ${successes + failures} metadata records"
  }

  private boolean hasImage(metadataContent) {
    def metadataMap = xmlSlurper.parseText(metadataContent)
    def idInfo = metadataMap.identificationInfo.MD_DataIdentification
    def thumbnail = idInfo.graphicOverview.MD_BrowseGraphic.fileName.CharacterString.text()
    return thumbnail?.trim()
  }
}

class GeoportalLoader extends Loader {
  GeoportalLoader(Iterator<String> metadataUrls, String geoportalEndpoint) {
    super(metadataUrls, geoportalEndpoint)
    this.target.auth.basic('gptadmin', 'gptadmin')
    this.httpMethod = Method.PUT
  }
}
