import groovy.json.JsonSlurper
import groovyx.net.http.HTTPBuilder
import groovyx.net.http.Method

def parentIdentifier

buildscript {
  repositories {
    mavenCentral()
  }
  dependencies {
    classpath 'org.codehaus.groovy.modules.http-builder:http-builder:0.7'
  }
}

def sources = [
    [
        name             : 'GptCollections',
        class            : GeoportalExtractor,
        collectionsUrl   : 'http://data.nodc.noaa.gov/geoportal/rest/find/document',
        collectionsParams: [searchText: 'title%3A*%20NOT%20title%3AGHRSST'], //All collections not part of GHRSST
        requireBrowseImg : true,
        granules         : false
    ],
    [
        name             : 'GHRSST',
        class            : GeoportalExtractor,
        collectionsUrl   : 'http://data.nodc.noaa.gov/geoportal/rest/find/document',
        collectionsParams: [searchText: 'title:GHRSST%20NOT%20title:Documentation'],
        requireBrowseImg : false,
        granules         : [
          url      : 'http://www.nodc.noaa.gov/search/granule/rest/find/document',
          params   : [searchText: "fileIdentifier:${-> parentIdentifier}*"],
        ]
    ],
    [
        name             : 'COOPS',
        class            : GeoportalExtractor,
        collectionsUrl   : 'http://data.nodc.noaa.gov/geoportal/rest/find/document',
        collectionsParams: [searchText: 'title:NWLON%20NOT%20title:Documentation'],
        requireBrowseImg : false,
        granules         : [
          url      : 'http://www.nodc.noaa.gov/search/granule/rest/find/document',
          params   : [searchText: 'fileIdentifier%3ACO-OPS*', after: '2016-06-30'],
        ]
    ],
    [
        name             : 'DEM',
        class            : WafExtractor,
        collectionsUrl   : 'http://ngdc.noaa.gov/metadata/published/NOAA/OneStop/DEM/iso/xml/',
        collectionsParams: [:],
        requireBrowseImg : false,
        granules         : false
    ],
    [
        name: 'SAMOS',
        class: GeoportalExtractor,
        collectionsUrl: 'http://data.nodc.noaa.gov/geoportal/rest/find/document',
        collectionsParams: [searchText: 'title:SAMOS'],
        requireBrowseImg : false,
        granules         : [
          url: 'http://data.nodc.noaa.gov/nodc/archive/metadata/test/granule/iso/samos/',
          params: [:],
          class: WafExtractor,
        ]
    ],
    [
        name: 'WOA13',
        class: GeoportalExtractor,
        collectionsUrl: 'http://data.nodc.noaa.gov/geoportal/rest/find/document',
        collectionsParams: [searchText: 'fileIdentifier:"gov.noaa.nodc:0114815"'],
        requireBrowseImg : false,
        granules         : [
          url: 'http://data.nodc.noaa.gov/nodc/archive/metadata/test/granule/iso/woa13/',
          params: [:],
          class: WafExtractor,
        ]
    ]
]

def targets = [
    [
        name : 'LocalApi',
        class: Loader,
        url  : 'http://localhost:8097/onestop/api/metadata'
    ],
    [
        name : 'CUApi',
        class: Loader,
        url  : 'https://sciapps.colorado.edu/onestop/api/metadata'
    ],
    [
        name : 'LocalGeoportal',
        class: GeoportalLoader,
        url  : "http://localhost:8888/rest/metadata/item"
    ],
    [
        name : 'LocalDisk',
        class: Loader,
        filePath  : "/tmp"
    ]
]

// Return a closure resource for loading sources into each target
def loadToTarget( Map source, Map target ) {
  return {
    // Load collections first
    def extractor = source.class.newInstance(source.collectionsUrl, source.collectionsParams)
    def filePath = target.filePath
    def loader
    if (filePath){
      loader = target.class.newInstance(extractor, source.name, filePath, new File(filePath))
    } else {
      loader = target.class.newInstance(extractor, new HTTPBuilder(target.url))
    }
    loader.load(source.requireBrowseImg)

    // Load granules if they exist
    def granulesAttrs = source.granules
    if (granulesAttrs) {
      def collections = loader.collections
      collections.each { i ->
        // ParentID is only relevant to GHRSST but set here to avoid duplicate code below since params
        // is dynamically constructed for each collection
        def parentIdentifier = i

        if (granulesAttrs.class) {
          extractor = granulesAttrs.class.newInstance(granulesAttrs.url, granulesAttrs.params)
        } else {
          extractor = source.class.newInstance(granulesAttrs.url, granulesAttrs.params)
        }
        if (filePath){
          loader = target.class.newInstance(extractor, source.name, filePath, new File(filePath))
        } else {
          loader = target.class.newInstance(extractor, new HTTPBuilder(granulesAttrs.url))
        }
        if (source.name == 'GHRSST') {
          loader.load(parentIdentifier)
        } else {
          loader.load(source.requireBrowseImg)
        }
      }
    }
  }
}

// Creates a task to load from each source into local & server targets
targets.each { target ->
  tasks.create("etlAllTo${target.name}")
    .dependsOn( sources.collect { source ->
      tasks.create("etl${source.name}To${target.name}")
        .doFirst(loadToTarget(source, target))
  })
}

class WafExtractor implements Iterator<String> {
  private String endpointUrl
  private Map searchParams
  private Iterator wafIterator

  WafExtractor(String endpointUrl, Map searchParams = [:]) {
    this.endpointUrl = endpointUrl
    this.searchParams = searchParams

    def html = new HTTPBuilder(endpointUrl).get(searchParams)
    def xmlFiles = html.'**'.collect({ "${endpointUrl}/${it}" }).findAll({ it.endsWith('.xml') })
    this.wafIterator = xmlFiles.iterator()
  }

  @Override
  boolean hasNext() {
    return wafIterator.hasNext()
  }

  @Override
  String next() {
    return wafIterator.next()
  }
}


class GeoportalExtractor implements Iterator<String> {
  private String endpointUrl
  private Map searchParams
  private Integer pageSize = 250
  private List currentPage = []
  private Integer currentPageNum = 0
  private Integer currentIndex = 0
  private JsonSlurper jsonSlurper = new JsonSlurper()
  private gptPageRequests = 0
  private maxGptPageRequests = 2 //TODO Should be an arg passed in; Update me as needed!

  GeoportalExtractor(String endpointUrl, Map searchParams = [:]) {
    this.endpointUrl = endpointUrl
    this.searchParams = searchParams
    getNextPage()
  }

  @Override
  boolean hasNext() {
    if (currentIndex < currentPage.size()) {
      return true
    } else {
      gptPageRequests++
      println "Number of gptPageRequests: ${gptPageRequests}"
      if (gptPageRequests < maxGptPageRequests) {
        getNextPage()
        return currentPage.size() > 0
      } else {
        return false
      }
    }
  }

  @Override
  String next() {
    return hasNext() ? currentPage[currentIndex++] : null
  }

  private getNextPage() {
    try {
      currentPage = jsonSlurper.parse(buildUrl()).records.collect { record ->
        record.links.find({ it.type.contains('metadata') }).href
      }
    } catch (e) {
      println "getNextPage() failed. Go to next page..."
      sleep(3000)
    }
    currentIndex = 0
    currentPageNum++
  }

  private buildUrl() {
    def params = searchParams + [start: currentPageNum * pageSize, max: pageSize, f: 'json', orderBy: 'title']
    def queryString = params.collect({ k, v -> k + '=' + v }).join('&')
    def url = endpointUrl + '?' + queryString
    println url
    return new URL(url)
  }
}

class Loader {
  protected Iterator<String> metadataUrls
  protected Method httpMethod
  def xmlSlurper, target, parentDir, metadataContent
  def successes = 0
  def failures = 0
  def collections = []

  Loader(Iterator<String> metadataUrls, HTTPBuilder target) {
    init(metadataUrls)
    this.target = target
    target.ignoreSSLIssues()
  }

  Loader(Iterator<String> metadataUrls, String sourceName, String targetPath, File target) {
    init(metadataUrls)
    (parentDir = new File("$targetPath/$sourceName")).mkdirs()
    this.target = target
  }

  def init( Iterator<String> metadataUrls ) {
    httpMethod = Method.POST
    this.target = target
    this.metadataUrls = metadataUrls
    xmlSlurper = new XmlSlurper()
  }

  void load(boolean requireBrowseImg) {
    (parentDir = new File((File)parentDir, "collections")).mkdirs()
    metadataUrls.eachWithIndex { metadata, i ->
      try {
        metadataContent = new URL(metadata).text
      } catch (e) {
        println "Error downloading metadata #${i} from ${metadata}: ${e.message}"
        return // break out of current loop
      }
      writeToDestination(metadataContent, requireBrowseImg, true)
    }
  }

  void load(String parentIdentifier) {
    (parentDir = new File((File)parentDir, "granules")).mkdirs()
    def fragment = xmlSlurper.parseText("""<gmd:parentIdentifier>
    <gco:CharacterString>gov.noaa.nodc:${parentIdentifier}</gco:CharacterString>
  </gmd:parentIdentifier>""")
    metadataUrls.eachWithIndex { metadata, i ->
      try {
        metadataContent = new URL(metadata).text
        writeToDestination(metadataContent)
      } catch (e) {
        println "Error downloading metadata #${i} from ${metadata}: ${e.message}"
        return // break out of current loop
      }
    }
  }

  private void writeToDestination(metadataContent, requireBrowseImg=false, isCollection=false) {
    def metadataMap = xmlSlurper.parseText(metadataContent)
    if (!requireBrowseImg || hasImage(metadataMap)) {
      if (target in HTTPBuilder) {
        sendToApi(metadataContent, isCollection)
      } else { // Target is file
        try {
          new File(parentDir, "/${fileName(metadataMap)}") << metadataContent
          successes++
        } catch (Exception ex) {
          println "Failed to write to file, exception: $ex"
          failures++
        }
      }
      println "${successes} of ${successes + failures} metadata records written"
    }
  }

  private sendToApi(dataContent, isCollection) {
    target.request(httpMethod) {
      requestContentType = 'application/xml'
      body = dataContent
      response.success = { resp, data ->
        successes++
        println "${data.data.id} created: ${data.data.attributes.created}"
        if (isCollection) { collections.add(data.data.id.replace('gov.noaa.nodc:', '')) }
      }
      response.failure = { resp, data ->
        failures++
        println "Error uploading metadata: ${data}"
      }
    }
  }

  private boolean hasImage(metadataMap) {
    def idInfo = metadataMap.identificationInfo.MD_DataIdentification
    def thumbnail = idInfo.graphicOverview.MD_BrowseGraphic.fileName.CharacterString.text()
    return thumbnail?.trim()
  }

  private String fileName(metadataMap) {
    def fileName = metadataMap.fileIdentifier.CharacterString
    return "${fileName}.xml"
  }
}

class GeoportalLoader extends Loader {
  GeoportalLoader(Iterator<String> metadataUrls, String geoportalEndpoint) {
    super(metadataUrls, new HTTPBuilder(geoportalEndpoint))
    this.target.auth.basic('gptadmin', 'gptadmin')
    this.httpMethod = Method.PUT
  }
}
